# agents/response_generator.py

from langchain_openai import ChatOpenAI
from langchain_core.prompts import ChatPromptTemplate
from dotenv import load_dotenv
import os
import logging
from rich.console import Console

console = Console()
load_dotenv()

# Initialize the logger
logger = logging.getLogger(__name__)
logger.setLevel(logging.DEBUG)
# Initialize the OpenAI LLM with LangChain
llm = ChatOpenAI(temperature=0.5, model_name="gpt-4o")

def generate_final_response(question, query_result, summary):
    logger.info("Generating final response.")
    prompt = ChatPromptTemplate.from_messages(
        [
        (
            "system",
            """
            You are an AI assistant. Given the user's question, the query result, and a summary of the dataset, provide a concise and helpful answer.
            If users, asked for help in a decision, you should finally offer a concrete answer.
            It might the query result be just a single number or simple string not a dataframe, in that case the answer of user qustion should be generated by that result.
            If user question is about specific district, you should tell user that this answere is about that district.
            Usually all needed data is provided in query result, investigate in detail.
            Your answers should be data-driven and mention the usfull information about the dataset.
            Your answers should not just general points for the question. It should be data-driven and mention the usfull information about the dataset.
            Your answers should include numbers and statistics if possible.
            Importatnt: Sometime users question is about fractions or percentages, in that case the query result is just a number and you should generate the answere based on that number.
                Query Result:
                {query_result}

                Dataset Summary:
                {summary}

                Answer:
            """,
        ),
        ("human", "{question}"),
        ]
    )


    chain = prompt | llm 
    _input = {
        "question": question,
        "query_result": str(query_result),
        "summary": summary
    }
    response = chain.invoke(_input)

    logger.debug(f"LLM response: {response.content}")

    final_response = response.content
    logger.info("Final response generated.")
    return final_response
